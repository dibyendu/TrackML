{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXBusmrp1vaL"
   },
   "source": [
    "# Find Noisy Nodes in Graph without Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Graph Features: `[0]`\n",
    "  - Node Features: `[x, y] + [0, 1, 1, 1, 0, 0]` i.e. coordinate + layers\n",
    "  \n",
    "\n",
    "  - Graph Labels: `[`_Fraction of noisy nodes_`]`\n",
    "  - Node Labels: `[0, 1]` for noisy and `[1, 0]` for **NOT** noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "tjd3-8PJdK2m"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "PATH_PREFIX = '../'\n",
    "\n",
    "import sys\n",
    "sys.path.append(PATH_PREFIX)\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "import model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from graphGenerator import to_graph_dict_without_edges\n",
    "\n",
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "code_folding": [
     0,
     3,
     8,
     56,
     83,
     105,
     129,
     168,
     176,
     181
    ],
    "colab": {},
    "colab_type": "code",
    "id": "TrGithqWUML7"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "\n",
    "\n",
    "def get_node_pos(graph):\n",
    "    \"\"\"Return a `dict` of node: np.array([x, y]) pairs from a graph.\"\"\"\n",
    "    return {k: np.array([v['features'][0], v['features'][1]]) for k, v in graph.node.items()}\n",
    "\n",
    "\n",
    "def generate_random_graph(rand,\n",
    "                          num_nodes_min_max,\n",
    "                          dimensions=2,\n",
    "                          noisy_nodes_percentage=25):\n",
    "    \"\"\"Creates a complete directed graph.\n",
    "\n",
    "  The graphs are geographic threshold graphs, but with added edges via a\n",
    "  minimum spanning tree algorithm, to ensure all nodes are connected.\n",
    "\n",
    "  Args:\n",
    "    rand: A random seed for the graph generator. Default= None.\n",
    "    num_nodes_min_max: A sequence [lower, upper) number of nodes per graph.\n",
    "    dimensions: (optional) An `int` number of dimensions for the positions.\n",
    "      Default= 2.\n",
    "  Returns:\n",
    "    The directed graph.\n",
    "  \"\"\"\n",
    "    num_nodes = rand.randint(*num_nodes_min_max)\n",
    "\n",
    "    pos_array = rand.uniform(size=(num_nodes, dimensions))\n",
    "    layers = np.random.randint(2, size=(num_nodes, 6, 2)).astype(float)\n",
    "    noise = np.full(num_nodes, False, dtype=bool)\n",
    "    idx = np.arange(num_nodes)\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:int(num_nodes * noisy_nodes_percentage / 100)]\n",
    "    noise[idx] = True\n",
    "    \n",
    "    input_graph_node_features = np.concatenate((pos_array, np.reshape(layers, (num_nodes, 12))), axis=1)\n",
    "    target_graph_node_features = np.eye(2)[noise.astype(int)]\n",
    "    \n",
    "    input_graph_dict = {\n",
    "        \"globals\": np.array([0.0]),\n",
    "        \"nodes\": input_graph_node_features,\n",
    "        \"edges\": np.array([[]]),\n",
    "        \"senders\": np.array([]),\n",
    "        \"receivers\": np.array([])\n",
    "    }\n",
    "    target_graph_dict = {\n",
    "        \"globals\": np.array([np.sum(noise).astype(float) / num_nodes]),\n",
    "        \"nodes\": target_graph_node_features,\n",
    "        \"edges\": np.array([[]]),\n",
    "        \"senders\": np.array([]),\n",
    "        \"receivers\": np.array([])\n",
    "    }\n",
    "    \n",
    "    return input_graph_dict, target_graph_dict\n",
    "\n",
    "\n",
    "def generate_dict_graphs(rand, num_examples, num_nodes_min_max):\n",
    "    \"\"\"Generate graphs for training.\n",
    "\n",
    "  Args:\n",
    "    rand: A random seed (np.RandomState instance).\n",
    "    num_examples: Total number of graphs to generate.\n",
    "    num_nodes_min_max: A 2-tuple with the [lower, upper) number of nodes per\n",
    "      graph. The number of nodes for a graph is uniformly sampled within this\n",
    "      range.\n",
    "\n",
    "  Returns:\n",
    "    input_graphs: The list of input graphs.\n",
    "    target_graphs: The list of output graphs.\n",
    "    graphs: The list of generated graphs.\n",
    "  \"\"\"\n",
    "    input_graphs = []\n",
    "    target_graphs = []\n",
    "    for _ in range(num_examples):\n",
    "        input_graph_dict, target_graph_dict = generate_random_graph(rand, num_nodes_min_max)\n",
    "        \n",
    "        # input_graph_dict, target_graph_dict = to_graph_dict_without_edges(data)\n",
    "\n",
    "        input_graphs.append(input_graph_dict)\n",
    "        target_graphs.append(target_graph_dict)\n",
    "    return input_graphs, target_graphs\n",
    "\n",
    "\n",
    "def create_placeholders(rand, batch_size, num_nodes_min_max):\n",
    "    \"\"\"Creates placeholders for the model training and evaluation.\n",
    "\n",
    "  Args:\n",
    "    rand: A random seed (np.RandomState instance).\n",
    "    batch_size: Total number of graphs per batch.\n",
    "    num_nodes_min_max: A 2-tuple with the [lower, upper) number of nodes per\n",
    "      graph. The number of nodes for a graph is uniformly sampled within this\n",
    "      range.\n",
    "\n",
    "  Returns:\n",
    "    input_ph: The input graph's placeholders, as a graph namedtuple.\n",
    "    target_ph: The target graph's placeholders, as a graph namedtuple.\n",
    "  \"\"\"\n",
    "    # Create some example data for inspecting the vector sizes.\n",
    "    input_graphs, target_graphs = generate_dict_graphs(rand, batch_size, num_nodes_min_max)\n",
    "    input_ph = utils_tf.placeholders_from_data_dicts(input_graphs)\n",
    "    target_ph = utils_tf.placeholders_from_data_dicts(target_graphs)\n",
    "\n",
    "    return input_ph, target_ph\n",
    "\n",
    "\n",
    "def create_feed_dict(rand, batch_size, num_nodes_min_max, input_ph, target_ph):\n",
    "    \"\"\"Creates placeholders for the model training and evaluation.\n",
    "\n",
    "  Args:\n",
    "    rand: A random seed (np.RandomState instance).\n",
    "    batch_size: Total number of graphs per batch.\n",
    "    num_nodes_min_max: A 2-tuple with the [lower, upper) number of nodes per\n",
    "      graph. The number of nodes for a graph is uniformly sampled within this\n",
    "      range.\n",
    "    input_ph: The input graph's placeholders, as a graph namedtuple.\n",
    "    target_ph: The target graph's placeholders, as a graph namedtuple.\n",
    "\n",
    "  Returns:\n",
    "    feed_dict: The feed `dict` of input and target placeholders and data.\n",
    "    raw_graphs: The `dict` of raw networkx graphs.\n",
    "  \"\"\"\n",
    "    inputs, targets = generate_dict_graphs(rand, batch_size, num_nodes_min_max)\n",
    "    input_graphs = utils_np.data_dicts_to_graphs_tuple(inputs)\n",
    "    target_graphs = utils_np.data_dicts_to_graphs_tuple(targets)\n",
    "    feed_dict = {input_ph: input_graphs, target_ph: target_graphs}\n",
    "\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def compute_accuracy(target,\n",
    "                     output,\n",
    "                     use_only_noisy=False):\n",
    "    \"\"\"Calculate model accuracy.\n",
    "\n",
    "  Returns the number of correctly predicted noisy nodes and the number\n",
    "  of completely solved graphs (100% correct predictions).\n",
    "\n",
    "  Args:\n",
    "    target: A `graphs.GraphsTuple` that contains the target graphs.\n",
    "    output: A `graphs.GraphsTuple` that contains the output graphs.\n",
    "    use_only_noisy: A `bool` indicator of whether to consider\n",
    "                    only noisy nodes for computing accuracy or not.\n",
    "\n",
    "  Returns:\n",
    "    correct: A `float` fraction of correctly labeled nodes/edges.\n",
    "    solved: A `float` fraction of graphs that are completely correctly labeled.\n",
    "  \"\"\"\n",
    "    tdds = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "    odds = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "    cs = []\n",
    "    ss = []\n",
    "    for td, od in zip(tdds, odds):\n",
    "        xn = np.argmax(td[\"nodes\"], axis=-1)\n",
    "        yn = np.argmax(od[\"nodes\"], axis=-1)\n",
    "        c = []\n",
    "        if use_only_noisy:\n",
    "            c.append((xn == yn)[xn > 0])\n",
    "        else:\n",
    "            c.append(xn == yn)\n",
    "        c = np.concatenate(c, axis=0)\n",
    "        s = np.all(c)\n",
    "        cs.append(c)\n",
    "        ss.append(s)\n",
    "    correct = np.mean(np.concatenate(cs, axis=0))\n",
    "    solved = np.mean(np.stack(ss))\n",
    "    return correct, solved\n",
    "\n",
    "\n",
    "def create_loss_ops(target_op, output_ops):\n",
    "    loss_ops = [\n",
    "        tf.losses.softmax_cross_entropy(target_op.nodes, output_op.nodes)\n",
    "        for output_op in output_ops\n",
    "    ]\n",
    "    return loss_ops\n",
    "\n",
    "\n",
    "def make_all_runnable_in_session(*args):\n",
    "    \"\"\"Lets an iterable of TF graphs be output from a session as NP graphs.\"\"\"\n",
    "    return [utils_tf.make_runnable_in_session(a) for a in args]\n",
    "\n",
    "\n",
    "class GraphPlotter(object):\n",
    "    def __init__(self, ax, graph, pos):\n",
    "        self._ax = ax\n",
    "        self._graph = graph\n",
    "        self._pos = pos\n",
    "        self._base_draw_kwargs = dict(G=self._graph,\n",
    "                                      pos=self._pos,\n",
    "                                      ax=self._ax)\n",
    "        self._nodes = None\n",
    "        self._noisy_nodes = None\n",
    "        self._noisy_nodes_count = None\n",
    "        self._ax.set_axis_off()\n",
    "\n",
    "    @property\n",
    "    def noisy_nodes_count(self):\n",
    "        if self._noisy_nodes_count is None:\n",
    "            self._noisy_nodes_count = len(self.noisy_nodes)\n",
    "        return self._noisy_nodes_count\n",
    "\n",
    "    @property\n",
    "    def nodes(self):\n",
    "        if self._nodes is None:\n",
    "            self._nodes = self._graph.nodes()\n",
    "        return self._nodes\n",
    "\n",
    "    @property\n",
    "    def noisy_nodes(self):\n",
    "        if self._noisy_nodes is None:\n",
    "            self._noisy_nodes = [\n",
    "                n for n in self.nodes\n",
    "                if np.all(self._graph.node[n].get('features') == np.array([0, 1]).astype(float))\n",
    "            ]\n",
    "        return self._noisy_nodes\n",
    "\n",
    "    def _make_draw_kwargs(self, **kwargs):\n",
    "        kwargs.update(self._base_draw_kwargs)\n",
    "        return kwargs\n",
    "\n",
    "    def _draw(self, draw_function, zorder=None, **kwargs):\n",
    "        draw_kwargs = self._make_draw_kwargs(**kwargs)\n",
    "        collection = draw_function(**draw_kwargs)\n",
    "        if collection is not None and zorder is not None:\n",
    "            try:\n",
    "                # This is for compatibility with older matplotlib.\n",
    "                collection.set_zorder(zorder)\n",
    "            except AttributeError:\n",
    "                # This is for compatibility with newer matplotlib.\n",
    "                collection[0].set_zorder(zorder)\n",
    "        return collection\n",
    "\n",
    "    def draw_nodes(self, **kwargs):\n",
    "        \"\"\"Useful kwargs: nodelist, node_size, node_color, linewidths.\"\"\"\n",
    "        if (\"node_color\" in kwargs\n",
    "                and isinstance(kwargs[\"node_color\"], collections.Sequence)\n",
    "                and len(kwargs[\"node_color\"]) in {3, 4}\n",
    "                and not isinstance(kwargs[\"node_color\"][0],\n",
    "                                   (collections.Sequence, np.ndarray))):\n",
    "            num_nodes = len(kwargs.get(\"nodelist\", self.nodes))\n",
    "            kwargs[\"node_color\"] = np.tile(\n",
    "                np.array(kwargs[\"node_color\"])[None], [num_nodes, 1])\n",
    "        return self._draw(nx.draw_networkx_nodes, **kwargs)\n",
    "\n",
    "    def draw_graph_with_noise(self,\n",
    "                              node_size=200,\n",
    "                              node_color=(1.0, 1.0, 1.0),\n",
    "                              noisy_node_color=(1.0, 0.0, 0.0),\n",
    "                              node_linewidth=1.0,\n",
    "                              draw_noisy_nodes=True):\n",
    "        node_border_color = (0.0, 0.0, 0.0, 1.0)\n",
    "        if isinstance(node_color, dict):\n",
    "            c = [node_color[n] for n in self.nodes]\n",
    "        else:\n",
    "            c = node_color\n",
    "        # Plot nodes.\n",
    "        self.draw_nodes(nodelist=self.nodes,\n",
    "                        node_size=node_size,\n",
    "                        node_color=c,\n",
    "                        linewidths=node_linewidth,\n",
    "                        edgecolors=node_border_color,\n",
    "                        zorder=20)\n",
    "        # Plot noisy nodes.\n",
    "        if draw_noisy_nodes:\n",
    "            self.draw_nodes(nodelist=self.noisy_nodes,\n",
    "                            node_size=node_size,\n",
    "                            node_color=noisy_node_color,\n",
    "                            linewidths=node_linewidth,\n",
    "                            edgecolors=node_border_color,\n",
    "                            zorder=30)\n",
    "        self._ax.set_title(\"Total noisy nodes: {}\".format(self.noisy_nodes_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "6oEV1OC3UQAc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@title Visualize example graphs\n",
    "\n",
    "seed = 1\n",
    "rand = np.random.RandomState(seed=seed)\n",
    "\n",
    "num_examples = 4  #10\n",
    "num_nodes_min_max = (8, 12)\n",
    "\n",
    "input_ph, target_ph = create_placeholders(rand, num_examples, num_nodes_min_max)\n",
    "feed_dict = create_feed_dict(rand, num_examples, num_nodes_min_max, input_ph, target_ph)\n",
    "input_graphs_tuple, target_graphs_tuple = feed_dict[input_ph], feed_dict[target_ph]\n",
    "\n",
    "# We can visualize the graph using networkx.\n",
    "input_graphs_nx = utils_np.graphs_tuple_to_networkxs(input_graphs_tuple)\n",
    "target_graphs_nx = utils_np.graphs_tuple_to_networkxs(target_graphs_tuple)\n",
    "\n",
    "# print('====================================')\n",
    "\n",
    "# print(input_graphs_nx[0].graph)\n",
    "# print(input_graphs_nx[0].nodes)\n",
    "# print(input_graphs_nx[0].nodes.data())  # or   print(digraph.nodes(data=True))\n",
    "# print(input_graphs_nx[0].edges)\n",
    "# print(input_graphs_nx[0].edges.data())\n",
    "\n",
    "# print('=====================================')\n",
    "\n",
    "# print(target_graphs_nx[0].graph)\n",
    "# print(target_graphs_nx[0].nodes)\n",
    "# print(target_graphs_nx[0].nodes.data())  # or   print(digraph.nodes(data=True))\n",
    "# print(target_graphs_nx[0].edges)\n",
    "# print(target_graphs_nx[0].edges.data())\n",
    "\n",
    "# print('====================================')\n",
    "\n",
    "w = 4\n",
    "h = int(np.ceil(num_examples / w))\n",
    "fig = plt.figure(40, figsize=(w * 4, h * 4))\n",
    "fig.clf()\n",
    "for j, (input_graph, target_graph) in enumerate(zip(input_graphs_nx, target_graphs_nx)):\n",
    "    ax = fig.add_subplot(h, w, j + 1)\n",
    "    pos = get_node_pos(input_graph)\n",
    "    plotter = GraphPlotter(ax, target_graph, pos)\n",
    "    plotter.draw_graph_with_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "cY09Bll0vuVj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@title Set up model training and evaluation\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "seed = 2\n",
    "rand = np.random.RandomState(seed=seed)\n",
    "\n",
    "# Model parameters.\n",
    "# Number of processing (message-passing) steps.\n",
    "num_processing_steps = 50\n",
    "\n",
    "# Data / training parameters.\n",
    "num_training_iterations = 100  #10000\n",
    "batch_size_tr = 32\n",
    "batch_size_ge = 100\n",
    "\n",
    "# Number of nodes per graph sampled uniformly from this range.\n",
    "num_nodes_min_max_tr = (8, 17)\n",
    "num_nodes_min_max_ge = (16, 33)\n",
    "\n",
    "# Data.\n",
    "# Input and target placeholders.\n",
    "input_ph, target_ph = create_placeholders(rand, batch_size_tr, num_nodes_min_max_tr)\n",
    "\n",
    "# Connect the data to the model.\n",
    "# Instantiate the model.\n",
    "model = model.EncodeProcessDecode(edge_output_size=None, node_output_size=2)\n",
    "# A list of outputs, one per processing step.\n",
    "output_ops_tr = model(input_ph, num_processing_steps)  # 50 `GraphsTuple` objects\n",
    "output_ops_ge = model(input_ph, num_processing_steps)  # 50 `GraphsTuple` objects\n",
    "\n",
    "# Training loss.\n",
    "loss_ops_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "# Average loss across processing steps.\n",
    "loss_op_tr = sum(loss_ops_tr) / num_processing_steps\n",
    "\n",
    "# Test/generalization loss.\n",
    "loss_ops_ge = create_loss_ops(target_ph, output_ops_ge)\n",
    "loss_op_ge = loss_ops_ge[-1]  # Loss from final processing step.\n",
    "\n",
    "# Optimizer.\n",
    "learning_rate = 1e-3\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "step_op = optimizer.minimize(loss_op_tr)\n",
    "\n",
    "# Lets an iterable of TF graphs be output from a session as NP graphs.\n",
    "input_ph, target_ph = make_all_runnable_in_session(input_ph, target_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "WoVdyUTjvzWb"
   },
   "outputs": [],
   "source": [
    "#@title Reset session\n",
    "\n",
    "# This cell resets the Tensorflow session, but keeps the same computational\n",
    "# graph.\n",
    "\n",
    "try:\n",
    "  sess.close()\n",
    "except NameError:\n",
    "  pass\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "last_iteration = 0\n",
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "losses_ge = []\n",
    "corrects_ge = []\n",
    "solveds_ge = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2941
    },
    "colab_type": "code",
    "id": "wWSqSYyQv0Ur",
    "outputId": "73e0c8d4-e1de-4525-cba9-19328b545956",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#@title Run training\n",
    "\n",
    "# You can interrupt this cell's training loop at any time, and visualize the\n",
    "# intermediate results by running the next cell (below). You can then resume\n",
    "# training by simply executing this cell again.\n",
    "\n",
    "# How much time between logging and printing the current results.\n",
    "log_every_seconds = 10\n",
    "\n",
    "print(\"# (iteration number), T (elapsed seconds), \"\n",
    "      \"Ltr (training loss), Lge (test/generalization loss), \"\n",
    "      \"Ctr (training fraction nodes/edges labeled correctly), \"\n",
    "      \"Str (training fraction examples solved correctly), \"\n",
    "      \"Cge (test/generalization fraction nodes/edges labeled correctly), \"\n",
    "      \"Sge (test/generalization fraction examples solved correctly)\")\n",
    "\n",
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "for iteration in range(last_iteration, num_training_iterations):\n",
    "    last_iteration = iteration\n",
    "    feed_dict = create_feed_dict(rand, batch_size_tr, num_nodes_min_max_tr, input_ph, target_ph)\n",
    "    train_values = sess.run(\n",
    "        {\n",
    "            \"step\": step_op,\n",
    "            \"target\": target_ph,\n",
    "            \"loss\": loss_op_tr,\n",
    "            \"outputs\": output_ops_tr\n",
    "        },\n",
    "        feed_dict=feed_dict)\n",
    "    the_time = time.time()\n",
    "    elapsed_since_last_log = the_time - last_log_time\n",
    "    if elapsed_since_last_log > log_every_seconds:\n",
    "        last_log_time = the_time\n",
    "        feed_dict = create_feed_dict(rand, batch_size_ge, num_nodes_min_max_ge, input_ph, target_ph)\n",
    "        test_values = sess.run(\n",
    "            {\n",
    "                \"target\": target_ph,\n",
    "                \"loss\": loss_op_ge,\n",
    "                \"inputs\": input_ph,\n",
    "                \"outputs\": output_ops_ge\n",
    "            },\n",
    "            feed_dict=feed_dict)\n",
    "        correct_tr, solved_tr = compute_accuracy(train_values[\"target\"],\n",
    "                                                 train_values[\"outputs\"][-1],\n",
    "                                                 use_only_noisy=False)\n",
    "        correct_ge, solved_ge = compute_accuracy(test_values[\"target\"],\n",
    "                                                 test_values[\"outputs\"][-1],\n",
    "                                                 use_only_noisy=True)\n",
    "        elapsed = time.time() - start_time\n",
    "        losses_tr.append(train_values[\"loss\"])\n",
    "        corrects_tr.append(correct_tr)\n",
    "        solveds_tr.append(solved_tr)\n",
    "        losses_ge.append(test_values[\"loss\"])\n",
    "        corrects_ge.append(correct_ge)\n",
    "        solveds_ge.append(solved_ge)\n",
    "        logged_iterations.append(iteration)\n",
    "        print(\"# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Ctr {:.4f}, Str\"\n",
    "              \" {:.4f}, Cge {:.4f}, Sge {:.4f}\".format(iteration, elapsed,\n",
    "                                                       train_values[\"loss\"],\n",
    "                                                       test_values[\"loss\"],\n",
    "                                                       correct_tr, solved_tr,\n",
    "                                                       correct_ge, solved_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1281
    },
    "colab_type": "code",
    "id": "u0ckrMtj72s-",
    "outputId": "10c7bbc1-a4ae-4ec9-e4df-1c4498c0dad4"
   },
   "outputs": [],
   "source": [
    "#@title Visualize results\n",
    "\n",
    "# This cell visualizes the results of training. You can visualize the\n",
    "# intermediate results by interrupting execution of the cell above, and running\n",
    "# this cell. You can then resume training by simply executing the above cell\n",
    "# again.\n",
    "\n",
    "\n",
    "def softmax_prob_last_dim(x):\n",
    "    e = np.exp(x)\n",
    "    return e[:, -1] / np.sum(e, axis=-1)\n",
    "\n",
    "\n",
    "# Plot results curves.\n",
    "fig = plt.figure(1, figsize=(18, 3))\n",
    "fig.clf()\n",
    "x = np.array(logged_iterations)\n",
    "# Loss.\n",
    "y_tr = losses_tr\n",
    "y_ge = losses_ge\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "ax.plot(x, y_tr, \"k\", label=\"Training\")\n",
    "ax.plot(x, y_ge, \"k--\", label=\"Test/generalization\")\n",
    "ax.set_title(\"Loss across training\")\n",
    "ax.set_xlabel(\"Training iteration\")\n",
    "ax.set_ylabel(\"Loss (binary cross-entropy)\")\n",
    "ax.legend()\n",
    "# Correct.\n",
    "y_tr = corrects_tr\n",
    "y_ge = corrects_ge\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "ax.plot(x, y_tr, \"k\", label=\"Training\")\n",
    "ax.plot(x, y_ge, \"k--\", label=\"Test/generalization\")\n",
    "ax.set_title(\"Fraction correct across training\")\n",
    "ax.set_xlabel(\"Training iteration\")\n",
    "ax.set_ylabel(\"Fraction nodes/edges correct\")\n",
    "# Solved.\n",
    "y_tr = solveds_tr\n",
    "y_ge = solveds_ge\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "ax.plot(x, y_tr, \"k\", label=\"Training\")\n",
    "ax.plot(x, y_ge, \"k--\", label=\"Test/generalization\")\n",
    "ax.set_title(\"Fraction solved across training\")\n",
    "ax.set_xlabel(\"Training iteration\")\n",
    "ax.set_ylabel(\"Fraction examples solved\")\n",
    "\n",
    "\n",
    "\n",
    "# Plot graphs and results after each processing step.\n",
    "# Predicted noisy nodes are colored\n",
    "# from red to yellow to green, where red means the model is confident the node is\n",
    "# noisy, green means the model is confident the node is NOT noisy,\n",
    "# and yellowish colors mean the model isn't sure.\n",
    "\n",
    "max_graphs_to_plot = 4 #6\n",
    "num_steps_to_plot = 4\n",
    "node_size = 120\n",
    "min_c = 0.3\n",
    "targets = utils_np.graphs_tuple_to_networkxs(test_values[\"target\"])\n",
    "inputs = utils_np.graphs_tuple_to_networkxs(test_values[\"inputs\"])\n",
    "step_indices = np.floor(\n",
    "    np.linspace(0, num_processing_steps - 1,\n",
    "                num_steps_to_plot)).astype(int).tolist()\n",
    "outputs = list(\n",
    "    zip(*(utils_np.graphs_tuple_to_data_dicts(test_values[\"outputs\"][i])\n",
    "          for i in step_indices)))\n",
    "h = max_graphs_to_plot\n",
    "w = num_steps_to_plot + 1\n",
    "fig = plt.figure(101, figsize=(18, h * 3))\n",
    "fig.clf()\n",
    "ncs = []\n",
    "for j, (target, inp, output) in enumerate(zip(targets, inputs, outputs)):\n",
    "    if j >= h:\n",
    "        break\n",
    "    # Ground truth.\n",
    "    iax = j * (1 + num_steps_to_plot) + 1\n",
    "    ax = fig.add_subplot(h, w, iax)\n",
    "    pos = get_node_pos(inp)    \n",
    "    plotter = GraphPlotter(ax, target, pos)\n",
    "    plotter.draw_graph_with_noise(node_size=node_size)\n",
    "    ax.set_axis_on()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    try:\n",
    "        ax.set_facecolor([0.9] * 3 + [1.0])\n",
    "    except AttributeError:\n",
    "        ax.set_axis_bgcolor([0.9] * 3 + [1.0])\n",
    "    ax.grid(None)\n",
    "    ax.set_title(\"Ground truth\\nNoisy nodes: {}\".format(\n",
    "        plotter.noisy_nodes_count))\n",
    "    # Prediction.\n",
    "    for k, outp in enumerate(output):\n",
    "        iax = j * (1 + num_steps_to_plot) + 2 + k\n",
    "        ax = fig.add_subplot(h, w, iax)\n",
    "        plotter = GraphPlotter(ax, target, pos)\n",
    "        color = {}\n",
    "        prob = softmax_prob_last_dim(outp[\"nodes\"])\n",
    "        for i, n in enumerate(plotter.nodes):\n",
    "            color[n] = np.array([prob[n], 1.0 - prob[n], 0.0, 1.0\n",
    "                                 ]) * (1.0 - min_c) + min_c\n",
    "        plotter.draw_graph_with_noise(node_size=node_size, node_color=color, draw_noisy_nodes=False)\n",
    "        ax.set_title(\"Model-predicted\\nStep {:02d} / {:02d}\".format(\n",
    "            step_indices[k] + 1, step_indices[-1] + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#@title Print the Nth graph\n",
    "\n",
    "N_th_graph = 0\n",
    "\n",
    "target = test_values[\"target\"]\n",
    "output = test_values[\"outputs\"][-1]\n",
    "\n",
    "tdds = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "odds = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "\n",
    "td, od = tdds[N_th_graph], odds[N_th_graph]\n",
    "\n",
    "xn = np.argmax(td[\"nodes\"], axis=-1)\n",
    "yn = np.argmax(od[\"nodes\"], axis=-1)\n",
    "\n",
    "print(od['nodes'])\n",
    "print(xn)\n",
    "print(yn)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "shortest_path.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "154px",
    "left": "1078px",
    "right": "20px",
    "top": "111px",
    "width": "353px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
