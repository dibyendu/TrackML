{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXBusmrp1vaL"
   },
   "source": [
    "# Find Noisy Nodes in Graph without Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features:\n",
    "  - Graph : `[0]`\n",
    "  - Node  : `[x, y] + [0, 1, 1, 1, 0, 0]` i.e. coordinate + layers\n",
    "  \n",
    "#### Labels:\n",
    "  - Graph : $\\left[\\frac{\\# \\text{ Noisy Nodes}}{\\# \\text{ Nodes}}\\right]$\n",
    "  - Node  : `[0, 1]` for noisy and `[1, 0]` for **NOT** noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:08:02.659248Z",
     "start_time": "2019-07-13T16:08:02.652731Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_PREFIX = '../'\n",
    "DATA_DIR = 'D0toPiKInAcceptanceSignal_Iteration1_Parsed/'\n",
    "MODEL_SAVE_DIR = 'Model(Noise)/'\n",
    "\n",
    "import sys\n",
    "sys.path.append(PATH_PREFIX)\n",
    "\n",
    "DATA_FILE_PATH = PATH_PREFIX + DATA_DIR + 'dataset_{:04d}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:08:05.857955Z",
     "start_time": "2019-07-13T16:08:03.462381Z"
    },
    "cellView": "form",
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "tjd3-8PJdK2m"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "from graphGenerator import to_graph_dict_without_edges\n",
    "\n",
    "import collections\n",
    "import ujson\n",
    "import time\n",
    "\n",
    "from graph_nets import utils_np, utils_tf\n",
    "import modelNoise as model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:08:08.391572Z",
     "start_time": "2019-07-13T16:08:08.155300Z"
    },
    "cellView": "form",
    "code_folding": [
     0,
     3,
     7,
     28,
     48,
     70,
     109,
     117,
     122
    ],
    "colab": {},
    "colab_type": "code",
    "id": "TrGithqWUML7"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions\n",
    "\n",
    "\n",
    "def get_node_pos(graph):\n",
    "    return {k: np.array([v['features'][0], v['features'][1]]) for k, v in graph.node.items()}\n",
    "\n",
    "\n",
    "def generate_dict_graphs(raw_data, offset, batch_size):\n",
    "    \"\"\"Generate graphs for training.\n",
    "    \n",
    "    Args:\n",
    "        raw_data: The list of raw data objects.\n",
    "        offset: Starting index to pick from the `raw_data`.\n",
    "        batch_size: Total number of graphs per batch.\n",
    "        \n",
    "    Returns:\n",
    "        input_graph_dict: An input graph in dictionary format.\n",
    "        target_graph_dict: A labelled input graph in dictionary format.\n",
    "    \"\"\"\n",
    "    input_graphs = []\n",
    "    target_graphs = []\n",
    "    for i in range(offset, offset + batch_size):        \n",
    "        input_graph_dict, target_graph_dict = to_graph_dict_without_edges(raw_data[i])\n",
    "        input_graphs.append(input_graph_dict)\n",
    "        target_graphs.append(target_graph_dict)\n",
    "    return input_graphs, target_graphs\n",
    "\n",
    "\n",
    "def create_placeholders(raw_data, offset, batch_size):\n",
    "    \"\"\"Creates placeholders for the model training and evaluation.\n",
    "\n",
    "  Args:\n",
    "    raw_data: The list of raw data objects.\n",
    "    offset: Starting index to pick from the `raw_data`.\n",
    "    batch_size: Total number of graphs per batch.\n",
    "\n",
    "  Returns:\n",
    "    input_ph: The input graph's placeholders, as a graph namedtuple.\n",
    "    target_ph: The target graph's placeholders, as a graph namedtuple.\n",
    "  \"\"\"\n",
    "    # Create some example data for inspecting the vector sizes.\n",
    "    input_graphs, target_graphs = generate_dict_graphs(raw_data, offset, batch_size)\n",
    "    input_ph = utils_tf.placeholders_from_data_dicts(input_graphs)\n",
    "    target_ph = utils_tf.placeholders_from_data_dicts(target_graphs)\n",
    "\n",
    "    return input_ph, target_ph\n",
    "\n",
    "\n",
    "def create_feed_dict(raw_data, offset, batch_size, input_ph, target_ph):\n",
    "    \"\"\"Creates placeholders for the model training and evaluation.\n",
    "\n",
    "  Args:\n",
    "    raw_data: The list of raw data objects.\n",
    "    offset: Starting index to pick from the `raw_data`.\n",
    "    batch_size: Total number of graphs per batch.\n",
    "    input_ph: The input graph's placeholders, as a graph namedtuple.\n",
    "    target_ph: The target graph's placeholders, as a graph namedtuple.\n",
    "\n",
    "  Returns:\n",
    "    feed_dict: The feed `dict` of input and target placeholders and data.\n",
    "    raw_graphs: The `dict` of raw networkx graphs.\n",
    "  \"\"\"\n",
    "    inputs, targets = generate_dict_graphs(raw_data, offset, batch_size)\n",
    "    input_graphs = utils_np.data_dicts_to_graphs_tuple(inputs)\n",
    "    target_graphs = utils_np.data_dicts_to_graphs_tuple(targets)\n",
    "    feed_dict = {input_ph: input_graphs, target_ph: target_graphs}\n",
    "\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def compute_accuracy(target,\n",
    "                     output,\n",
    "                     use_only_noisy=False):\n",
    "    \"\"\"Calculate model accuracy.\n",
    "\n",
    "  Returns the number of correctly predicted noisy nodes and the number\n",
    "  of completely solved graphs (100% correct predictions).\n",
    "\n",
    "  Args:\n",
    "    target: A `graphs.GraphsTuple` that contains the target graphs.\n",
    "    output: A `graphs.GraphsTuple` that contains the output graphs.\n",
    "    use_only_noisy: A `bool` indicator of whether to consider\n",
    "                    only noisy nodes for computing accuracy or not.\n",
    "\n",
    "  Returns:\n",
    "    correct: A `float` fraction of correctly labeled nodes/edges.\n",
    "    solved: A `float` fraction of graphs that are completely correctly labeled.\n",
    "  \"\"\"\n",
    "    tdds = utils_np.graphs_tuple_to_data_dicts(target)\n",
    "    odds = utils_np.graphs_tuple_to_data_dicts(output)\n",
    "    cs = []\n",
    "    ss = []\n",
    "    for td, od in zip(tdds, odds):\n",
    "        xn = np.argmax(td[\"nodes\"], axis=-1)\n",
    "        yn = np.argmax(od[\"nodes\"], axis=-1)\n",
    "        c = []\n",
    "        if use_only_noisy:\n",
    "            c.append((xn == yn)[xn > 0])\n",
    "        else:\n",
    "            c.append(xn == yn)\n",
    "        c = np.concatenate(c, axis=0)\n",
    "        s = np.all(c)\n",
    "        cs.append(c)\n",
    "        ss.append(s)\n",
    "    correct = np.mean(np.concatenate(cs, axis=0))\n",
    "    solved = np.mean(np.stack(ss))\n",
    "    return correct, solved\n",
    "\n",
    "\n",
    "def create_loss_ops(target_op, output_ops):\n",
    "    loss_ops = [\n",
    "        tf.losses.softmax_cross_entropy(target_op.nodes, output_op.nodes)\n",
    "        for output_op in output_ops\n",
    "    ]\n",
    "    return loss_ops\n",
    "\n",
    "\n",
    "def make_all_runnable_in_session(*args):\n",
    "    \"\"\"Lets an iterable of TF graphs be output from a session as NP graphs.\"\"\"\n",
    "    return [utils_tf.make_runnable_in_session(a) for a in args]\n",
    "\n",
    "\n",
    "class GraphPlotter(object):\n",
    "    def __init__(self, ax, graph, pos):\n",
    "        self._ax = ax\n",
    "        self._graph = graph\n",
    "        self._pos = pos\n",
    "        self._base_draw_kwargs = dict(G=self._graph,\n",
    "                                      pos=self._pos,\n",
    "                                      ax=self._ax)\n",
    "        self._nodes = None\n",
    "        self._noisy_nodes = None\n",
    "        self._noisy_nodes_count = None\n",
    "        self._ax.set_axis_off()\n",
    "\n",
    "    @property\n",
    "    def noisy_nodes_count(self):\n",
    "        if self._noisy_nodes_count is None:\n",
    "            self._noisy_nodes_count = len(self.noisy_nodes)\n",
    "        return self._noisy_nodes_count\n",
    "\n",
    "    @property\n",
    "    def nodes(self):\n",
    "        if self._nodes is None:\n",
    "            self._nodes = self._graph.nodes()\n",
    "        return self._nodes\n",
    "\n",
    "    @property\n",
    "    def noisy_nodes(self):\n",
    "        if self._noisy_nodes is None:\n",
    "            self._noisy_nodes = [\n",
    "                n for n in self.nodes\n",
    "                if np.all(self._graph.node[n].get('features') == np.array([0, 1]).astype(float))\n",
    "            ]\n",
    "        return self._noisy_nodes\n",
    "\n",
    "    def _make_draw_kwargs(self, **kwargs):\n",
    "        kwargs.update(self._base_draw_kwargs)\n",
    "        return kwargs\n",
    "\n",
    "    def _draw(self, draw_function, zorder=None, **kwargs):\n",
    "        draw_kwargs = self._make_draw_kwargs(**kwargs)\n",
    "        collection = draw_function(**draw_kwargs)\n",
    "        if collection is not None and zorder is not None:\n",
    "            try:\n",
    "                # This is for compatibility with older matplotlib.\n",
    "                collection.set_zorder(zorder)\n",
    "            except AttributeError:\n",
    "                # This is for compatibility with newer matplotlib.\n",
    "                collection[0].set_zorder(zorder)\n",
    "        return collection\n",
    "\n",
    "    def draw_nodes(self, **kwargs):\n",
    "        \"\"\"Useful kwargs: nodelist, node_size, node_color, linewidths.\"\"\"\n",
    "        if (\"node_color\" in kwargs\n",
    "                and isinstance(kwargs[\"node_color\"], collections.Sequence)\n",
    "                and len(kwargs[\"node_color\"]) in {3, 4}\n",
    "                and not isinstance(kwargs[\"node_color\"][0],\n",
    "                                   (collections.Sequence, np.ndarray))):\n",
    "            num_nodes = len(kwargs.get(\"nodelist\", self.nodes))\n",
    "            kwargs[\"node_color\"] = np.tile(\n",
    "                np.array(kwargs[\"node_color\"])[None], [num_nodes, 1])\n",
    "        return self._draw(nx.draw_networkx_nodes, **kwargs)\n",
    "\n",
    "    def draw_graph_with_noise(self,\n",
    "                              node_size=40,\n",
    "                              node_color=(1.0, 1.0, 1.0),\n",
    "                              noisy_node_color=(1.0, 0.0, 0.0),\n",
    "                              node_linewidth=1.0,\n",
    "                              draw_noisy_nodes=True):\n",
    "        node_border_color = (0.0, 0.0, 0.0, 1.0)\n",
    "        if isinstance(node_color, dict):\n",
    "            c = [node_color[n] for n in self.nodes]\n",
    "        else:\n",
    "            c = node_color\n",
    "        # Plot nodes.\n",
    "        self.draw_nodes(nodelist=self.nodes,\n",
    "                        node_size=node_size,\n",
    "                        node_color=c,\n",
    "                        linewidths=node_linewidth,\n",
    "                        edgecolors=node_border_color,\n",
    "                        zorder=20)\n",
    "        # Plot noisy nodes.\n",
    "        if draw_noisy_nodes:\n",
    "            self.draw_nodes(nodelist=self.noisy_nodes,\n",
    "                            node_size=node_size,\n",
    "                            node_color=noisy_node_color,\n",
    "                            linewidths=node_linewidth,\n",
    "                            edgecolors=node_border_color,\n",
    "                            zorder=30)\n",
    "        self._ax.set_title(\"Total noisy nodes: {}\".format(self.noisy_nodes_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:08:10.305694Z",
     "start_time": "2019-07-13T16:08:09.421082Z"
    },
    "cellView": "both",
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "6oEV1OC3UQAc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@title Visualize example graphs\n",
    "\n",
    "DATA_FILE_NUMBER = 1\n",
    "num_examples = 4\n",
    "\n",
    "\n",
    "data_file = open(DATA_FILE_PATH.format(DATA_FILE_NUMBER), 'r')\n",
    "input_raw_data = ujson.loads(data_file.read())\n",
    "data_file.close()\n",
    "\n",
    "input_ph, target_ph = create_placeholders(input_raw_data, 0, num_examples)\n",
    "feed_dict = create_feed_dict(input_raw_data, 0, num_examples, input_ph, target_ph)\n",
    "input_graphs_tuple, target_graphs_tuple = feed_dict[input_ph], feed_dict[target_ph]\n",
    "\n",
    "# We can visualize the graph using networkx.\n",
    "input_graphs_nx = utils_np.graphs_tuple_to_networkxs(input_graphs_tuple)\n",
    "target_graphs_nx = utils_np.graphs_tuple_to_networkxs(target_graphs_tuple)\n",
    "\n",
    "# print('====================================')\n",
    "\n",
    "# print(input_graphs_nx[0].graph)\n",
    "# print(input_graphs_nx[0].nodes)\n",
    "# print(input_graphs_nx[0].nodes.data())  # or   print(digraph.nodes(data=True))\n",
    "# print(input_graphs_nx[0].edges)\n",
    "# print(input_graphs_nx[0].edges.data())\n",
    "\n",
    "# print('=====================================')\n",
    "\n",
    "# print(target_graphs_nx[0].graph)\n",
    "# print(target_graphs_nx[0].nodes)\n",
    "# print(target_graphs_nx[0].nodes.data())  # or   print(digraph.nodes(data=True))\n",
    "# print(target_graphs_nx[0].edges)\n",
    "# print(target_graphs_nx[0].edges.data())\n",
    "\n",
    "# print('====================================')\n",
    "\n",
    "w = 4\n",
    "h = int(np.ceil(num_examples / w))\n",
    "fig = plt.figure(40, figsize=(w * 4, h * 4))\n",
    "fig.clf()\n",
    "for j, (input_graph, target_graph) in enumerate(zip(input_graphs_nx, target_graphs_nx)):\n",
    "    ax = fig.add_subplot(h, w, j + 1)\n",
    "    pos = get_node_pos(input_graph)\n",
    "    plotter = GraphPlotter(ax, target_graph, pos)\n",
    "    plotter.draw_graph_with_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:08:32.159544Z",
     "start_time": "2019-07-13T16:08:20.677688Z"
    },
    "cellView": "both",
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "cY09Bll0vuVj",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#@title Set up model training and evaluation\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "DATA_FILE_NUMBER_TR = 1\n",
    "DATA_FILE_NUMBER_GE = 80\n",
    "\n",
    "# Model parameters.\n",
    "# Number of processing (message-passing) steps.\n",
    "num_processing_steps = 10\n",
    "\n",
    "# Training parameters.\n",
    "num_training_iterations = 42  #10000\n",
    "batch_size_tr = 25\n",
    "batch_size_ge = 50\n",
    "\n",
    "# Data.\n",
    "training_raw_data = ujson.loads(open(DATA_FILE_PATH.format(DATA_FILE_NUMBER_TR), 'r').read())\n",
    "test_raw_data = ujson.loads(open(DATA_FILE_PATH.format(DATA_FILE_NUMBER_GE), 'r').read())\n",
    "\n",
    "# Input and target placeholders.\n",
    "input_ph, target_ph = create_placeholders(training_raw_data, 0, batch_size_tr)\n",
    "\n",
    "# Connect the data to the model.\n",
    "# Instantiate the model.\n",
    "model = model.EncodeProcessDecode(edge_output_size=None, node_output_size=2)\n",
    "# A list of outputs, one per processing step.\n",
    "output_ops_tr = model(input_ph, num_processing_steps)  # 10 `GraphsTuple` objects\n",
    "output_ops_ge = model(input_ph, num_processing_steps)  # 10 `GraphsTuple` objects\n",
    "\n",
    "# Training loss.\n",
    "loss_ops_tr = create_loss_ops(target_ph, output_ops_tr)\n",
    "# Average loss across processing steps.\n",
    "loss_op_tr = sum(loss_ops_tr) / num_processing_steps\n",
    "\n",
    "# Test/generalization loss.\n",
    "loss_ops_ge = create_loss_ops(target_ph, output_ops_ge)\n",
    "loss_op_ge = loss_ops_ge[-1]  # Loss from final processing step.\n",
    "\n",
    "# Optimizer.\n",
    "learning_rate = 1e-3\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "step_op = optimizer.minimize(loss_op_tr)\n",
    "\n",
    "# Lets an iterable of TF graphs be output from a session as NP graphs.\n",
    "input_ph, target_ph = make_all_runnable_in_session(input_ph, target_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:08:43.785882Z",
     "start_time": "2019-07-13T16:08:42.131429Z"
    },
    "cellView": "both",
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "WoVdyUTjvzWb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#@title Reset session\n",
    "\n",
    "# This cell resets the Tensorflow session, but keeps the same computational graph.\n",
    "\n",
    "try:\n",
    "  sess.close()\n",
    "except NameError:\n",
    "  pass\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "logged_iterations = []\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "losses_ge = []\n",
    "corrects_ge = []\n",
    "solveds_ge = []\n",
    "training_data_file_current_offset, test_data_file_current_offset = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:10:17.054885Z",
     "start_time": "2019-07-13T16:08:45.473156Z"
    },
    "cellView": "both",
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2941
    },
    "colab_type": "code",
    "id": "wWSqSYyQv0Ur",
    "outputId": "73e0c8d4-e1de-4525-cba9-19328b545956",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#@title Run training\n",
    "\n",
    "# How much time between logging and printing the current results.\n",
    "log_every_seconds = 1\n",
    "\n",
    "print(\"# (iteration number), T (elapsed seconds), \"\n",
    "      \"Ltr (training loss), Lge (test/generalization loss), \"\n",
    "      \"Ctr (training fraction nodes/edges labeled correctly), \"\n",
    "      \"Str (training fraction examples solved correctly), \"\n",
    "      \"Cge (test/generalization fraction nodes/edges labeled correctly), \"\n",
    "      \"Sge (test/generalization fraction examples solved correctly)\")\n",
    "\n",
    "start_time = time.time()\n",
    "last_log_time = start_time\n",
    "for iteration in range(num_training_iterations):\n",
    "    feed_dict = create_feed_dict(training_raw_data, training_data_file_current_offset, batch_size_tr, input_ph, target_ph)\n",
    "    train_values = sess.run(\n",
    "        {\n",
    "            \"step\": step_op,\n",
    "            \"target\": target_ph,\n",
    "            \"loss\": loss_op_tr,\n",
    "            \"outputs\": output_ops_tr\n",
    "        },\n",
    "        feed_dict=feed_dict)\n",
    "    the_time = time.time()\n",
    "    elapsed_since_last_log = the_time - last_log_time\n",
    "    if elapsed_since_last_log > log_every_seconds:\n",
    "        last_log_time = the_time\n",
    "        feed_dict = create_feed_dict(test_raw_data, test_data_file_current_offset, batch_size_ge, input_ph, target_ph)\n",
    "        test_values = sess.run(\n",
    "            {\n",
    "                \"target\": target_ph,\n",
    "                \"loss\": loss_op_ge,\n",
    "                \"inputs\": input_ph,\n",
    "                \"outputs\": output_ops_ge\n",
    "            },\n",
    "            feed_dict=feed_dict)\n",
    "        test_data_file_current_offset += batch_size_ge\n",
    "        test_data_file_current_offset %= 100\n",
    "        if test_data_file_current_offset == 0:\n",
    "            DATA_FILE_NUMBER_GE += 1\n",
    "            test_raw_data = ujson.loads(open(DATA_FILE_PATH.format(DATA_FILE_NUMBER_GE), 'r').read())\n",
    "        correct_tr, solved_tr = compute_accuracy(train_values[\"target\"],\n",
    "                                                 train_values[\"outputs\"][-1],\n",
    "                                                 use_only_noisy=False)\n",
    "        correct_ge, solved_ge = compute_accuracy(test_values[\"target\"],\n",
    "                                                 test_values[\"outputs\"][-1],\n",
    "                                                 use_only_noisy=True)\n",
    "        elapsed = time.time() - start_time\n",
    "        losses_tr.append(train_values[\"loss\"])\n",
    "        corrects_tr.append(correct_tr)\n",
    "        solveds_tr.append(solved_tr)\n",
    "        losses_ge.append(test_values[\"loss\"])\n",
    "        corrects_ge.append(correct_ge)\n",
    "        solveds_ge.append(solved_ge)\n",
    "        logged_iterations.append(iteration)\n",
    "        print(\"# {:05d}, T {:.1f}, Ltr {:.4f}, Lge {:.4f}, Ctr {:.4f}, Str\"\n",
    "              \" {:.4f}, Cge {:.4f}, Sge {:.4f}\".format(iteration, elapsed,\n",
    "                                                       train_values[\"loss\"],\n",
    "                                                       test_values[\"loss\"],\n",
    "                                                       correct_tr, solved_tr,\n",
    "                                                       correct_ge, solved_ge))\n",
    "    training_data_file_current_offset += batch_size_tr\n",
    "    training_data_file_current_offset %= 100\n",
    "    if training_data_file_current_offset == 0:\n",
    "        DATA_FILE_NUMBER_TR += 1\n",
    "        training_raw_data = ujson.loads(open(DATA_FILE_PATH.format(DATA_FILE_NUMBER_TR), 'r').read())\n",
    "\n",
    "save_path = saver.save(sess, PATH_PREFIX + MODEL_SAVE_DIR + 'model.ckpt')\n",
    "print('Model saved in path: %s' % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graphs and results after each processing step\n",
    "\n",
    "### Predicted noisy nodes are colored in <span style=\"color:red\">red</span>\n",
    "  - where more <span style=\"color:red\">reddish</span> means the model is more confident that the node is **noisy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T16:11:16.838522Z",
     "start_time": "2019-07-13T16:11:13.485461Z"
    },
    "cellView": "form",
    "code_folding": [
     0,
     8
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1281
    },
    "colab_type": "code",
    "id": "u0ckrMtj72s-",
    "outputId": "10c7bbc1-a4ae-4ec9-e4df-1c4498c0dad4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#@title Visualize results\n",
    "\n",
    "# This cell visualizes the results of training. You can visualize the\n",
    "# intermediate results by interrupting execution of the cell above, and running\n",
    "# this cell. You can then resume training by simply executing the above cell\n",
    "# again.\n",
    "\n",
    "\n",
    "def softmax_prob_last_dim(x):\n",
    "    e = np.exp(x)\n",
    "    return e[:, -1] / np.sum(e, axis=-1)\n",
    "\n",
    "\n",
    "# Plot results curves.\n",
    "fig = plt.figure(1, figsize=(18, 3))\n",
    "fig.clf()\n",
    "x = np.array(logged_iterations)\n",
    "# Loss.\n",
    "y_tr = losses_tr\n",
    "y_ge = losses_ge\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(x, y_tr, \"k\", label=\"Training Data\")\n",
    "ax.plot(x, y_ge, \"k--\", label=\"Validation Data\")\n",
    "ax.set_title(f\"Binary Cross Entropy Loss for {num_training_iterations - 2} Iterations\")\n",
    "ax.set_xlabel(\"Training Iteration\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend()\n",
    "# Correct.\n",
    "y_tr = corrects_tr\n",
    "y_ge = corrects_ge\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(x, y_tr, \"k\", label=\"Training Data\")\n",
    "ax.plot(x, y_ge, \"k--\", label=\"Validation Data\")\n",
    "ax.set_title(\"Ratio of Correctly Predicted Noisy Nodes\")\n",
    "ax.set_xlabel(\"Training Iteration\")\n",
    "ax.set_ylabel(\"Ratio\")\n",
    "ax.legend()\n",
    "# # Solved.\n",
    "# y_tr = solveds_tr\n",
    "# y_ge = solveds_ge\n",
    "# ax = fig.add_subplot(1, 3, 3)\n",
    "# ax.plot(x, y_tr, \"k\", label=\"Training\")\n",
    "# ax.plot(x, y_ge, \"k--\", label=\"Validation\")\n",
    "# ax.set_title(\"Fraction completely solved across training\")\n",
    "# ax.set_xlabel(\"Training iteration\")\n",
    "# ax.set_ylabel(\"Fraction examples solved\")\n",
    "\n",
    "plt.savefig('loss.svg', format='svg')\n",
    "\n",
    "\n",
    "\n",
    "# Plot graphs and results after each processing step.\n",
    "# Predicted noisy nodes are colored\n",
    "# from red to yellow to green, where red means the model is confident the node is\n",
    "# noisy, green means the model is confident the node is NOT noisy,\n",
    "# and yellowish colors mean the model isn't sure.\n",
    "\n",
    "max_graphs_to_plot = 4\n",
    "num_steps_to_plot = 4\n",
    "node_size = 40\n",
    "min_c = 0.3\n",
    "targets = utils_np.graphs_tuple_to_networkxs(test_values[\"target\"])\n",
    "inputs = utils_np.graphs_tuple_to_networkxs(test_values[\"inputs\"])\n",
    "step_indices = np.floor(np.linspace(0, num_processing_steps - 1, num_steps_to_plot)).astype(int).tolist()\n",
    "outputs = list(zip(*(utils_np.graphs_tuple_to_data_dicts(test_values[\"outputs\"][i]) for i in step_indices)))\n",
    "h = max_graphs_to_plot\n",
    "w = num_steps_to_plot + 1\n",
    "fig = plt.figure(101, figsize=(18, h * 3))\n",
    "fig.clf()\n",
    "ncs = []\n",
    "for j, (target, inp, output) in enumerate(zip(targets, inputs, outputs)):\n",
    "    if j >= h:\n",
    "        break\n",
    "    # Ground truth.\n",
    "    iax = j * (1 + num_steps_to_plot) + 1\n",
    "    ax = fig.add_subplot(h, w, iax)\n",
    "    pos = get_node_pos(inp)    \n",
    "    plotter = GraphPlotter(ax, target, pos)\n",
    "    plotter.draw_graph_with_noise(node_size=node_size)\n",
    "#     ax.set_axis_on()\n",
    "#     ax.set_xticks([])\n",
    "#     ax.set_yticks([])\n",
    "#     try:\n",
    "#         ax.set_facecolor([0.9] * 3 + [1.0])\n",
    "#     except AttributeError:\n",
    "#         ax.set_axis_bgcolor([0.9] * 3 + [1.0])\n",
    "    ax.grid(None)\n",
    "    ax.set_title(\"Ground Truth\\nNoisy nodes: {}\".format(plotter.noisy_nodes_count))\n",
    "    # Prediction.\n",
    "    for k, outp in enumerate(output):\n",
    "        iax = j * (1 + num_steps_to_plot) + 2 + k\n",
    "        ax = fig.add_subplot(h, w, iax)\n",
    "        plotter = GraphPlotter(ax, target, pos)\n",
    "        color = {}\n",
    "        prob = softmax_prob_last_dim(outp[\"nodes\"])\n",
    "        for i, n in enumerate(plotter.nodes):\n",
    "            color[n] = np.array([1.0, 1 - prob[n], 1 - prob[n]]) * (1.0 - min_c) + min_c\n",
    "        plotter.draw_graph_with_noise(node_size=node_size, node_color=color, draw_noisy_nodes=False)\n",
    "        ax.set_title(\"Model Prediction\\nStep {:02d} / {:02d}\".format(step_indices[k] + 1, step_indices[-1] + 1))\n",
    "\n",
    "plt.savefig('prediction.svg', format='svg')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "shortest_path.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "154px",
    "left": "1078px",
    "right": "20px",
    "top": "111px",
    "width": "353px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
